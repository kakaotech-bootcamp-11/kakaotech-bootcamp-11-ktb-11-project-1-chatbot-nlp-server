Index: app.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\nimport logging\nimport os\nfrom flask import Flask,  jsonify, Response, stream_with_context\nfrom flask_cors import CORS\nfrom document_retriever import my_retriever\nfrom error_handler import register_error_handlers\nfrom openai import OpenAIError\nfrom werkzeug.exceptions import BadRequest\nfrom conversation_history import save_conversation, history\nfrom pymongo import MongoClient\nfrom utils import get_request_data, topic_classification, handle_weather_topic, handle_trans_topic, handle_else_topic, text_chatgpt\nfrom mongo_client import get_mongo_client\nimport json\n\n\n\n# 플라스크 앱 정의\napp = Flask(__name__)\nCORS(app)\nregister_error_handlers(app) # flask error handler 등록\n\n# 로깅 설정\nlogging.basicConfig(\n    filename='./logging/error_log.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# 환경 변수에서 MongoDB 연결 URL 가져오기\n# mongo_uri = os.getenv('MONGO_URI')\nclient, db, collection = get_mongo_client()\n\n\n\n# 환경 변수에서 API 키와 PDF 경로를 로드\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nTMAP_API_KEY = os.getenv('TMAP_API_KEY')\nKAKAO_MAP_API_KEY = os.getenv('KAKAO_MAP_API_KEY1')\nWEATHER_API_KEY = os.getenv('WEATHER_API_KEY')\nLOCATION1 = os.getenv('LOCATION1')\n\n# LLM 변수 정의\nSTREAM_TOKEN_SIZE = 1 # 스트림 토큰 단위 default 125\nMODEL_VERSION = \"gpt-4o-mini\" # \"gpt-3.5-turbo\"\nMAX_TOKENS_OUTPUT = 500\n\n# 검색할 문서 로드\nfile_path = os.getenv('PDF_PATH', 'data/ktb_data_09.md')\ntry:\n    retriever, faiss_db = my_retriever(file_path)\nexcept OpenAIError as e:\n    raise e\nprint(\"=======검색기 로드 끝========\")\n\n# 모델의 응답을 스트리밍하기 위한 제너레이터 함수\ndef generate_response_stream(user_id, chat_id, user_input):\n    my_history = history(collection, user_id, chat_id, limit=4)\n\n    history_prompt = \"\"\n    if len(my_history) >  0: # 기존 대화 내역이 있음. \n\n        #history_prompt = \"이 질문에 답변하는데, 다음의 기존 대화 내역과 연관이 있으면, 다음의 기존 대화 내역을 참고해줘. 기존 대화 내역: \\n```\"\n\n        history_prompt = \"\"\"\n                        When answering this question, refer to the existing conversation history below if needed.\\n\",\n                        \"If the conversation history is not relevant or helpful for this question, proceed without referencing it.\\n\",\n                        \"existing conversation history: ```\n                        \"\"\"\n        for h in my_history:\n            history_prompt +=  h['role']+\":\"+h['text']+\"\\n\"\n        history_prompt += '```'\n    print(\"원래 사용자 인풋:\\n\", user_input, \"=\"*10)\n    print(\"히스토리 프롬프트:\\n\", history_prompt, \"=\"*10)\n\n    input_txt = user_input + history_prompt\n    # retriever의 스트리밍 응답을 처리 (pipeline.stream 사용)\n    answer_text = ''\n    for chunk in retriever.stream(input_txt):  # stream을 사용하여 스트리밍 처리\n        print(\"chunk:\", chunk)\n        answer_text += chunk\n        chunk_json = json.dumps({\"text\": chunk}, ensure_ascii=False)\n        yield f\"data: {chunk_json}\\n\\n\" # \"data\": ... \\n\\n 을 \n        # print(chunk)\n    # 질문 & 응답 저장 \n    save_conversation(collection, user_id, chat_id, \"user\", user_input)\n    save_conversation(collection, user_id, chat_id, \"system\", answer_text)\n    print(\"최종 답변:\", answer_text)\n\n@app.route(\"/nlp-api/conv\", methods=['POST'])\ndef llm():\n    params = get_request_data() # request body 를 가져옴\n    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']\n    print(\"user_input, user_id, chat_id:\", user_input, user_id, chat_id)\n\n    #save_conversation(collection, user_id, chat_id, \"user\", user_input)\n\n    response_generator = generate_response_stream(user_id, chat_id, user_input)\n    return Response(stream_with_context(response_generator), mimetype='text/event-stream', )\n    #return Response(stream_message(response_generator), mimetype='application/json')\n\n@app.route(\"/nlp-api/title\", methods=['POST'])\ndef make_title(): # 대화의 타이틀 생성\n    params = get_request_data(title=True)\n    user_input = params['content']\n    system_prompt = \"\"\"넌 대화 타이틀을 만드는 역할이야. 챗봇에서 사용자의 첫 번째 메시지를 기반으로 해당 대화의 제목을 요약해줘.\"\"\"\n    title = text_chatgpt(system_prompt, user_input)\n\n    if title is None:\n        return jsonify({\"error\": \"죄송해요. 챗 지피티가 제목을 제대로 가져오지 못했어요.\"})\n    title = title.strip('\"') # 앞뒤의 큰 따옴표 제거\n    return jsonify({\"title\": title})\n\n@app.route(\"/nlp-api/test\", methods=['POST'])\ndef test(): # whole text 만든 다음, 청크 단위로 나눠 스트림 형식으로 전달\n    params = get_request_data() # request body 를 가져옴\n    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']\n    system_prompt = \"\"\"사용자의 질문에 친절하게 대답해줘.\"\"\"\n    result = text_chatgpt(system_prompt, user_input)\n    print(\"result(whole text):\", result)\n    response_generator = generate_response_stream(user_id, chat_id, user_input)\n    return Response(stream_message(response_generator), mimetype='text/event-stream')\n\n@app.route(\"/nlp-api/test/stream\", methods=['POST'])\ndef stream_output(): # chatGPT API 에서 실시간으로 청크 단위로 답변을 받아옴.\n    #user_input, user_id, chat_id = get_request_data()  # 공통\n    params = get_request_data() # request body 를 가져옴\n    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']\n\n    # 답변 가져오기\n    response_generator = generate_response_stream(user_id, chat_id, user_input)\n    return Response(stream_message(response_generator), mimetype='text/event-stream')\n\n# test function for error handling\n@app.route(\"/nlp-api/error_handling\", methods=['POST'])\ndef error_handle(): # 대화의 타이틀 생성 #(params)\n    params = get_request_data() # request body 를 가져옴\n    if not params : # json = {}\n        raise BadRequest(\"No request body\")\n    elif 'content' not in params or not params['content'].strip(): # json = {'msg': \"...\"} or json = {'content': \"\"}\n        raise BadRequest(\"No content field in request body or value for content is empty\")\n    return jsonify({\"result\": f\"no error:{params['content']}\"})\n\n\n\nif __name__ == '__main__':\n    print(\"app starts running\")\n    app.run(port=5001,debug=True)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app.py b/app.py
--- a/app.py	
+++ b/app.py	
@@ -1,25 +1,28 @@
-
+import openai
+import json
 import logging
+import warnings
 import os
-from flask import Flask,  jsonify, Response, stream_with_context
+from flask import Flask, request, jsonify, Response, abort
 from flask_cors import CORS
-from document_retriever import my_retriever
+from dotenv import load_dotenv
+from pdf_retriever import pdf_retriever
+from get_weather import get_weather_info
+from find_routes_v2 import get_route_description
 from error_handler import register_error_handlers
 from openai import OpenAIError
+
+
 from werkzeug.exceptions import BadRequest
-from conversation_history import save_conversation, history
-from pymongo import MongoClient
-from utils import get_request_data, topic_classification, handle_weather_topic, handle_trans_topic, handle_else_topic, text_chatgpt
-from mongo_client import get_mongo_client
-import json
-
-
 
 # 플라스크 앱 정의
 app = Flask(__name__)
 CORS(app)
-register_error_handlers(app) # flask error handler 등록
+register_error_handlers(app) # flask error handler 등록 
 
+# 모든 경고를 무시
+warnings.filterwarnings("ignore")
+
 # 로깅 설정
 logging.basicConfig(
     filename='./logging/error_log.log',
@@ -28,122 +31,230 @@
     datefmt='%Y-%m-%d %H:%M:%S'
 )
 
-# 환경 변수에서 MongoDB 연결 URL 가져오기
-# mongo_uri = os.getenv('MONGO_URI')
-client, db, collection = get_mongo_client()
-
 
-
-# 환경 변수에서 API 키와 PDF 경로를 로드
-OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
-TMAP_API_KEY = os.getenv('TMAP_API_KEY')
-KAKAO_MAP_API_KEY = os.getenv('KAKAO_MAP_API_KEY1')
-WEATHER_API_KEY = os.getenv('WEATHER_API_KEY')
-LOCATION1 = os.getenv('LOCATION1')
+# API 키 로드하기
+print("환경변수 로드 ")
+load_dotenv() # # .env 파일에서 환경 변수를 로드합니다
+OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY") # API 키 설정
+TMAP_API_KEY = os.environ.get("TMAP_API_KEY")
+KAKAO_MAP_API_KEY = os.environ.get("KAKAO_MAP_API_KEY1")
+WEATHER_API_KEY = os.environ.get("WEATHER_API_KEY")
 
-# LLM 변수 정의
-STREAM_TOKEN_SIZE = 1 # 스트림 토큰 단위 default 125
-MODEL_VERSION = "gpt-4o-mini" # "gpt-3.5-turbo"
+# LLM 변수 정의 
+STREAM_TOKEN_SIZE = 30
+MODEL_VERSION = "gpt-4o" # "gpt-3.5-turbo"  
 MAX_TOKENS_OUTPUT = 500
 
-# 검색할 문서 로드
-file_path = os.getenv('PDF_PATH', 'data/ktb_data_09.md')
-try:
-    retriever, faiss_db = my_retriever(file_path)
-except OpenAIError as e:
-    raise e
-print("=======검색기 로드 끝========")
-
-# 모델의 응답을 스트리밍하기 위한 제너레이터 함수
-def generate_response_stream(user_id, chat_id, user_input):
-    my_history = history(collection, user_id, chat_id, limit=4)
-
-    history_prompt = ""
-    if len(my_history) >  0: # 기존 대화 내역이 있음. 
+
+def split_text_into_tokens(text, max_tokens=STREAM_TOKEN_SIZE): # max_tokens : 스트림 1번에 보낼  토큰 단위를 지정 
+    # 텍스트를 공백을 기준으로 토큰화
+    words = text.split()
+    for i in range(0, len(words), max_tokens):
+        yield ' '.join(words[i:i+max_tokens])
+
+def stream_message(text, max_tokens=STREAM_TOKEN_SIZE, delay=1): # 데이터가 청크 단위로 스트리밍 된다. 
+    for chunk in split_text_into_tokens(text, max_tokens):
+        yield chunk  # 이 부분을 메시지 전송 로직으로 대체할 수 있습니다.
+
+
+def stream_chatgpt(system_prompt, user_prompt):
+    client = openai.OpenAI(api_key=OPENAI_API_KEY)
+    try:
+        response = client.chat.completions.create(
+            model=MODEL_VERSION,
+            messages=[
+                {"role": "system", "content": system_prompt + "\n 정보를 일반 텍스트로 작성해 주세요. 굵게 표시하지 말고, 특수 형식 없이 일반 텍스트로만 작성해 주세요."},
+                {"role": "user", "content": user_prompt}
+            ],
+            temperature=0.0, # 출력의 다양성 조절 (0~1), 높을 수록 창의적인 대답
+            max_tokens= MAX_TOKENS_OUTPUT, # 최대 출력 토큰 개수 
+            n = 1,         # 생성 답변 개수,
+            stream=True
+        )
+        def event_stream(): #stream generator
+            result_txt = ''
+            for chunk in response:
+                text = chunk.choices[0].delta.content
+                #print("chunk.choices[0]", chunk.choices[0])
+                #print("text:", text, "\n")
+                if text is None: # None type 일 경우 pass 
+                    continue
+                result_txt += str(text)
+
+                yield text
+            print(result_txt)
+            
+        return Response(event_stream(), mimetype='text/event-stream')
+    except OpenAIError as e:
+        logging.error(f"Error while calling chatGPT API function call: {str(e)}")
+        raise e # OpenAIError
+    except Exception as e:
+        logging.error(f"Error: {str(e)}")
+        raise e # OpenAIError
+    
+def text_chatgpt(system_prompt, user_prompt): # text 형식으로 리턴
+    client = openai.OpenAI(api_key=OPENAI_API_KEY)
+    try:
+        response = client.chat.completions.create(
+            model=MODEL_VERSION,
+            messages=[
+                {"role": "system", "content": system_prompt + "\n 정보를 일반 텍스트로 작성해 주세요. 굵게 표시하지 말고, 특수 형식 없이 일반 텍스트로만 작성해 주세요."},
+                {"role": "user", "content": user_prompt}
+            ],
+            temperature=0.0, # 출력의 다양성 조절 (0~1), 높을 수록 창의적인 대답
+            max_tokens= MAX_TOKENS_OUTPUT, # 최대 출력 토큰 개수 
+            n = 1,         # 생성 답변 개수,
+            stream=False
+        )
+        return response.choices[0].message.content
+    except OpenAIError as e:
+        logging.error(f"Error while calling chatGPT API function call: {str(e)}")
+        raise e # OpenAIError
+    except Exception as e:
+        logging.error(f"Error: {str(e)}")
+        raise e # OpenAIError
+    
 
-        #history_prompt = "이 질문에 답변하는데, 다음의 기존 대화 내역과 연관이 있으면, 다음의 기존 대화 내역을 참고해줘. 기존 대화 내역: \n```"
+def topic_classification(user_input):
+    system_prompt = """
+            You are a classifier. Your task is to analyze '{user_input}'.
+        - If '{user_input}' is a question about the asking weather, return 'WEATHER'.
+        - If '{user_input}' is a question about public transportation routes involving a specific origin and destination, return 'TRANS'.
+        - If '{user_input}' does not match either of the above cases, return 'ELSE'.
+        """
+    return text_chatgpt(system_prompt, user_input)
 
-        history_prompt = """
-                        When answering this question, refer to the existing conversation history below if needed.\n",
-                        "If the conversation history is not relevant or helpful for this question, proceed without referencing it.\n",
-                        "existing conversation history: ```
-                        """
-        for h in my_history:
-            history_prompt +=  h['role']+":"+h['text']+"\n"
-        history_prompt += '```'
-    print("원래 사용자 인풋:\n", user_input, "="*10)
-    print("히스토리 프롬프트:\n", history_prompt, "="*10)
+def extract_arrv_dest(user_input): #user input 에서 출발지와 도착지 출력  
+    system_prompt = """
+            Your task is to identify the departure and destination from the user's input. 
+            Follow these guidelines: 
+            1. If either the departure or destination is ambiguous or unclear, mark it as unknown. 
+            2. If the input refers to the user's current location, mark it as current.
+            3. If the input suggests the user's home location, mark it as home. 
+            4. Please return a dictionary formatted like this : {"from":departure, "to":destination}
+            """
+    return text_chatgpt(system_prompt =system_prompt, user_prompt= user_input )
 
-    input_txt = user_input + history_prompt
-    # retriever의 스트리밍 응답을 처리 (pipeline.stream 사용)
-    answer_text = ''
-    for chunk in retriever.stream(input_txt):  # stream을 사용하여 스트리밍 처리
-        print("chunk:", chunk)
-        answer_text += chunk
-        chunk_json = json.dumps({"text": chunk}, ensure_ascii=False)
-        yield f"data: {chunk_json}\n\n" # "data": ... \n\n 을 
-        # print(chunk)
-    # 질문 & 응답 저장 
-    save_conversation(collection, user_id, chat_id, "user", user_input)
-    save_conversation(collection, user_id, chat_id, "system", answer_text)
-    print("최종 답변:", answer_text)
+def handle_weather_topic(user_input):
+    weather_info = get_weather_info()
+    system_prompt = (f"You are a helpful assistant, and you will kindly answer questions about current weather. "
+              f"한국어로 대답해야해. 현재 날씨 정보는 다음과 같아. {weather_info}, "
+              "이 날씨 정보를 다 출력할 필요는 없고, 주어진 질문인 '{user_input}'에 필요한 답만 해줘 ")
+    result = stream_chatgpt(system_prompt, user_input)
+    return result
+    """if result is not None: return result # 비동기식 response 형식 
+    else:
+        result = "죄송해요. 챗 지피티가 답변을 가져오지 못했어요."
+        return Response(stream_message(result), content_type='text/plain')"""
 
-@app.route("/nlp-api/conv", methods=['POST'])
+def handle_trans_topic(user_input):
+    dict_string = extract_arrv_dest(user_input)
+    from_to_dict = json.loads(dict_string)
+    result_txt = get_route_description(from_to_dict, TMAP_API_KEY, KAKAO_MAP_API_KEY)
+    system_prompt = f"너는 출발지에서 목적지까지 경로를 안내하는 역할이고, 한국어로 대답해야해."\
+              f"사용자는 경로에 대해 요약된 텍스트를 줄거야. 너는 그걸 자연스럽게 만들어주면 돼. "\
+              f"출발지는 ```{from_to_dict['from']}```이고 목적지는 ```{from_to_dict['to']}```임.  "
+    user_prompt = f"다음을 자연스럽게 다시 말해줘:\n```{result_txt}``` "
+    return stream_chatgpt(user_prompt) 
+    """if dict_string:
+        from_to_dict = json.loads(dict_string)
+        result = get_route_description(from_to_dict, TMAP_API_KEY, KAKAO_MAP_API_KEY)
+    else:
+        result = "죄송해요. 다시 한 번 질문해주세요." #error msg
+        logging.error("extract_arrv_dest is None")
+
+    return Response(stream_message(result), content_type='text/plain')"""
+
+def handle_else_topic(user_input):
+    system_prompt = ("You are a helpful assistant."
+              "사용자들은 한국어로 질문할 거고, 너도 한국어로 대답해야돼")
+    result = stream_chatgpt(system_prompt, user_input)
+    return result
+    """if result is not None: 
+        return result # 비동기식 response 형식 
+    else:
+        result = "죄송해요. 챗 지피티가 답변을 가져오지 못했어요."
+        return Response(stream_message(result), content_type='text/plain')"""
+
+def validate_request_data():
+    params = request.get_json()
+    if not params:  # JSON 데이터가 없는 경우
+        raise BadRequest("No request body")
+    elif 'content' not in params or not params['content'].strip():  # 'content' 필드가 없거나 값이 비어 있는 경우
+        raise BadRequest("No content field in request body or value for content is empty")
+    return params
+
+@app.route("/conv", methods=['POST'])
 def llm():
-    params = get_request_data() # request body 를 가져옴
-    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']
-    print("user_input, user_id, chat_id:", user_input, user_id, chat_id)
-
-    #save_conversation(collection, user_id, chat_id, "user", user_input)
-
-    response_generator = generate_response_stream(user_id, chat_id, user_input)
-    return Response(stream_with_context(response_generator), mimetype='text/event-stream', )
-    #return Response(stream_message(response_generator), mimetype='application/json')
-
-@app.route("/nlp-api/title", methods=['POST'])
-def make_title(): # 대화의 타이틀 생성
-    params = get_request_data(title=True)
+    params = validate_request_data()  # 공통 함수 호출
     user_input = params['content']
-    system_prompt = """넌 대화 타이틀을 만드는 역할이야. 챗봇에서 사용자의 첫 번째 메시지를 기반으로 해당 대화의 제목을 요약해줘."""
-    title = text_chatgpt(system_prompt, user_input)
+
+    # 동기식으로 RAG 기법 적용한 QA 체인 생성
+    response = retriever(user_input)
+    print('RAG response:', response)
+    if response['result'] and not any(phrase in response['result'] for phrase in ["죄송", "모르겠습니다", "알 수 없습니다", "확인할 수 없습니다", "없습니다."])  : # 만약 
+        logging.info( f"RAG - user input: {user_input}")
+        print("logging: RAG 답변 ")
+        return Response(stream_message(response['result']), content_type='text/plain')
+    
+    elif not response['result']: #  # RAG를 수행하지 못했을 때 - 예외 처리 추가하기 
+        logging.error("error" "RAG를 요청 했으나 결과가 없음. 400")
+        raise BadRequest("No response from RAG") # 추후 수정
+
 
-    if title is None:
-        return jsonify({"error": "죄송해요. 챗 지피티가 제목을 제대로 가져오지 못했어요."})
-    title = title.strip('"') # 앞뒤의 큰 따옴표 제거
-    return jsonify({"title": title})
+    # 날씨, 교통, 그외 주제인지 분류하기 
+    topic = topic_classification(user_input)
+    print("topic:", topic)
+    if topic == "WEATHER":
+        return handle_weather_topic(user_input)
+    elif topic == "TRANS":
+        return handle_trans_topic(user_input)
+    elif topic == "ELSE":
+        return handle_else_topic(user_input)
+    """else:
+        logging.error("chat gpt failed to classify: result is None")
+        return jsonify({"error": "Topic classification failed"}), 500"""
 
-@app.route("/nlp-api/test", methods=['POST'])
-def test(): # whole text 만든 다음, 청크 단위로 나눠 스트림 형식으로 전달
-    params = get_request_data() # request body 를 가져옴
-    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']
+
+@app.route("/test", methods=['POST'])
+def test(): # whole text 만든 다음, 청크 단위로 나눠 스트림 형식으로 전달 
+    params = validate_request_data()
+    user_input = params['content'] 
     system_prompt = """사용자의 질문에 친절하게 대답해줘."""
     result = text_chatgpt(system_prompt, user_input)
-    print("result(whole text):", result)
-    response_generator = generate_response_stream(user_id, chat_id, user_input)
-    return Response(stream_message(response_generator), mimetype='text/event-stream')
+    print("result:", result)
+    return Response(stream_message(result), mimetype='text/event-stream') # 'text/plain'
 
-@app.route("/nlp-api/test/stream", methods=['POST'])
-def stream_output(): # chatGPT API 에서 실시간으로 청크 단위로 답변을 받아옴.
-    #user_input, user_id, chat_id = get_request_data()  # 공통
-    params = get_request_data() # request body 를 가져옴
-    user_input, user_id, chat_id = params['content'], params['user_id'], params['chat_id']
-
-    # 답변 가져오기
-    response_generator = generate_response_stream(user_id, chat_id, user_input)
-    return Response(stream_message(response_generator), mimetype='text/event-stream')
+@app.route("/test/stream", methods=['POST'])
+def stream_output(): # chatGPT API 에서 실시간으로 청크 단위로 답변을 받아옴. 
+    params = validate_request_data()
+    # 답변 가져오기 
+    user_input = params['content'] 
+    system_prompt = "You are a helpful assistant"
+    result = stream_chatgpt(system_prompt, user_input) # 
+    return result 
 
 # test function for error handling
-@app.route("/nlp-api/error_handling", methods=['POST'])
+@app.route("/error_handling", methods=['POST'])
 def error_handle(): # 대화의 타이틀 생성 #(params)
-    params = get_request_data() # request body 를 가져옴
+    params = request.get_json()
     if not params : # json = {}
         raise BadRequest("No request body")
     elif 'content' not in params or not params['content'].strip(): # json = {'msg': "..."} or json = {'content': ""}
         raise BadRequest("No content field in request body or value for content is empty")
+        #abort(500, description="No request body ---- ")
     return jsonify({"result": f"no error:{params['content']}"})
 
 
-
 if __name__ == '__main__':
-    print("app starts running")
-    app.run(port=5001,debug=True)
\ No newline at end of file
+    print("app.run 시작")
+    print("PDF 검색기 로드 시작")
+    pdf_path = './data/ktb_data_07_3.pdf'  # PDF 경로를 지정해주기 - 추후에 모든 pdf 읽도록  바꾸도록 지정하기 
+    retriever = pdf_retriever(pdf_path, MODEL_VERSION, OPENAI_API_KEY)
+    try:
+        retriever = pdf_retriever(pdf_path, MODEL_VERSION, OPENAI_API_KEY)
+    except OpenAIError as e:
+        raise e
+    print("PDF 검색기 로드 끝")
+    
+    app.run(port=5001,debug=True)
